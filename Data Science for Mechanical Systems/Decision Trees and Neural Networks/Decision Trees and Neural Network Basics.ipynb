{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1576dc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Local\\Temp\\ipykernel_15600\\4066392039.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28befe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5c26685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import graphviz\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc2422",
   "metadata": {},
   "source": [
    "<font size=\"5\">Problem 1</font> <br>\n",
    "**[Decision tree] Using the breast cancer dataset, let's encode the diagnosis = M  as 1, and diagnosis = B as 0, and treat this as a binary outcome (i.e., dependent variable).**\n",
    "1) [5 points] What is the value of the gini index of the full dataset? <br>\n",
    "2) [10 points] If we decide to use gini index as the error metric, and split on the feature of \"texture_mean\", what is value of the splitting point? (The correct answer falls in a range).<br>\n",
    "3) [10 points] If we want to repeat part 2, but using entropy as the error metric, what is the value of the splitting point? (The correct answer falls in a range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "538ed33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"breast_cancer_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95b3ac33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "389d012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.diagnosis = [1 if i==\"M\" else 0 for i in data.diagnosis] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac9c9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = data.drop(['id','diagnosis'],axis = 1)\n",
    "y_data = data.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90fd21e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train,y_test = train_test_split(X_data,data.diagnosis, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "744d6d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTree = tree.DecisionTreeClassifier(criterion=\"gini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2a7ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTree = DecisionTree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c092300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = DecisionTree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dea4cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(p: float):\n",
    "    \"\"\"Gini index for a given binary class ratio.\"\"\"\n",
    "    return 2 * p * (1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83a18579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1) The Gini Index for the entire dataset is : 0.4675300607546925\n"
     ]
    }
   ],
   "source": [
    "print(\"1.1) The Gini Index for the entire dataset is : {}\".format(gini_index(len(data[data.diagnosis == 0])/len(data)))) #arg for gini function is class_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTree_1b = tree.DecisionTreeClassifier(criterion=\"gini\",splitter=\"best\")\n",
    "\n",
    "X_data_1b = np.asarray(X_data.texture_mean).reshape(-1, 1)\n",
    "DecisionTree_1b = DecisionTree_1b.fit(X_data_1b,y_data)\n",
    "\n",
    "dot_data = tree.export_graphviz(\n",
    "    decision_tree=DecisionTree_1b, \n",
    "    out_file=None, \n",
    "    feature_names=['texture_mean'],#X_train.columns,  \n",
    "    class_names=[\"0\", \"1\"],  \n",
    "    filled=True, \n",
    "    rounded=True,  \n",
    "    special_characters=True,\n",
    "    max_depth=1,\n",
    ")  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render(\"cancer_tree\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1.2) The splitting point for texture_mean with gini index as the error criterion is : 19.47\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c44873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTree_1c = tree.DecisionTreeClassifier(criterion=\"entropy\",splitter=\"best\")\n",
    "\n",
    "X_data_1c = np.asarray(X_data.texture_mean).reshape(-1, 1)\n",
    "DecisionTree_1c = DecisionTree_1c.fit(X_data_1c,y_data)\n",
    "\n",
    "dot_data = tree.export_graphviz(\n",
    "    decision_tree=DecisionTree_1c, \n",
    "    out_file=None, \n",
    "    feature_names=['texture_mean'],#X_train.columns,  \n",
    "    class_names=[\"0\", \"1\"],  \n",
    "    filled=True, \n",
    "    rounded=True,  \n",
    "    special_characters=True,\n",
    "    max_depth=1,\n",
    ")  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render(\"cancer_tree\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8de59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1.3) The splitting point for texture_mean with entropy as the error criterion is : 18.635\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b37897",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "**[Neural Network] Using the breast cancer dataset, let's encode the diagnosis = M  as 1, and diagnosis = B as 0, and treat this as a binary outcome (i.e., dependent ariable). Let's use three features, namely,\"radius_extreme\", \"texture_extreme\", \"perimeter_extreme\", to build a neural network (NN). For this NN, let's have one hidden layer, together with the final layer (output layer). For the hidden layer, let's have 5 neurons, and using ReLU as the activation function. For the final layer, let's use sigmoid function as the activations.**<br>\n",
    "* 1. [5 points] How many parameters are there in this NN? <br>\n",
    "* 2. [10 points] Using the matrix notation convetion shown in the class, if we have both the matrices  and  filled with the value of 1, and  and  filled with the value of 0.1.After one forward parapagation, what is the value of the average prediction?<br> Note that if there are n  samples, the average prediction is taken over all n  samples.Please standardize the input (i.e., for each feature, subtract its mean, and divide by its standard deviation) before the forward propagation. <br>\n",
    "* 3. [5 points] Following part 2 above, if we use logloss as the error metric, what is the value of this error metric? Note here the entropy is also taken as the average over all n  samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ffba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, input_shape = (None, 3),activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5347eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"\\n2.1) Total number of parameters in the model is : 26\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a373b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the same dataset since the same attributes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"radius_extreme\",\n",
    "    \"texture_extreme\",\n",
    "    \"perimeter_extreme\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(data[features].values)\n",
    "X = scaler.transform(data[features].values)\n",
    "y = data.diagnosis.values.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass for a simple 2-layer NN, with 3 hidden units\n",
    "np.random.seed(10)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Calculates sigmoid function.\"\"\"\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "def relu(x):\n",
    "    \"\"\"Calculates relu function.\"\"\"\n",
    "    return np.maximum(0.0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96078abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the first layer\n",
    "W_1 = np.ones(shape=(5, X.shape[1]))\n",
    "print(f\"Shape of W_1 is {W_1.shape}\")\n",
    "\n",
    "b_1 = np.ones(shape=(5, 1))*0.1\n",
    "print(f\"Shape of b_1 is {b_1.shape}\")\n",
    "\n",
    "# parameters for the second layer\n",
    "W_2 = np.ones(shape=(1, 5))\n",
    "print(f\"Shape of W_2 is {W_2.shape}\")\n",
    "\n",
    "b_2 = np.ones(shape=(1, 1))*0.1\n",
    "print(f\"Shape of b_1 is {b_2.shape}\")\n",
    "\n",
    "# calculate the forward propagation\n",
    "Z_1 = X @ W_1.T\n",
    "print(f\"\\nShape of Z_1 is {Z_1.shape}\")\n",
    "print(\"Samples for Z_1:\")\n",
    "print(Z_1[:5])\n",
    "\n",
    "A_1 = relu(Z_1 + b_1.T) \n",
    "print(f\"Shape of A_1 is {A_1.shape}\")\n",
    "print(\"Samples for A_1:\")\n",
    "print(A_1[:5])\n",
    "\n",
    "Z_2 = A_1 @ W_2.T\n",
    "print(f\"\\nShape of Z_2 is {Z_2.shape}\")\n",
    "print(\"Samples for Z_2:\")\n",
    "print(Z_1[:5])\n",
    "\n",
    "A_2 = Y_hat = sigmoid(Z_2 + b_2.T)\n",
    "print(f\"Shape of A_2 is {A_2.shape}\")\n",
    "print(\"Samples for A_2:\")\n",
    "print(A_2[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952698f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prediction = sum((A_2)/A_2.shape[0])[0]\n",
    "print(\"2.2) The average prediction is : {}\".format(avg_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fbfd8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_loss = -np.mean(np.multiply(y, np.log(Y_hat+1E-16)) + np.multiply(1 - y, np.log(1 - Y_hat+1E-16)))\n",
    "print(\"2.3) The log loss error metric is : {}\".format(log_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6122ec",
   "metadata": {},
   "source": [
    "# Answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b81db",
   "metadata": {},
   "source": [
    "<font size=\"5\">Problem 1</font>\n",
    "\n",
    "1) The Gini Index for the entire dataset is : 0.4675300607546925\n",
    "\n",
    "2) The splitting point for texture_mean with gini index as the error criterion is : 19.47\n",
    "\n",
    "3) The splitting point for texture_mean with entropy as the error criterion is : 18.635\n",
    "\n",
    "<font size=\"5\">Problem 2</font>\n",
    "\n",
    "1) Total number of parameters in the model is : 26\n",
    "\n",
    "2) The average prediction after one pass is : 0.7134461335701471\n",
    "\n",
    "3) The log loss error metric is : 0.6811257843182167\n",
    "\n",
    "<font size=\"5\">Problem 3</font>\n",
    "\n",
    "Prove that, in Lloyd's algorithm for K-means clustering, for a given cluster  with \n",
    "observations, the averaged pairwise within-cluster squared L2 norm equals to the sum of\n",
    "squared L2 norm of each point (in the cluster) to its cluster center. That is:\n",
    "\n",
    "$\\frac{1}{n}\\sum_{i, j\\in{C}}\\left|\\left|x_{i}-x_{j}\\right|\\right|_{2}^2 = 2\\sum_{i \\in{C}}\\left|\\left|x_{i}-\\mu\\right|\\right|_{2}^2 ,$\n",
    "\n",
    "$where\\ \\ \\mu = \\frac{1}{n}\\sum_{i}x_{i}\\ \\ is\\ the\\ centroid\\ of\\ the\\ cluster.$\n",
    "\n",
    "\n",
    "<font size=\"4\">Proof : </font>\n",
    "\n",
    "$ We\\ know\\ that \\ \\ \\frac{1}{n}\\sum_{i}x_{i}\\ \\ is\\ the\\ centroid\\ of\\ the\\ cluster.$\n",
    "\n",
    "$K-Means:$\n",
    "\n",
    "$ a)\\ C_{1},C_{2},C_{3},.......,C_{k} = arg\\ min\\left(\\sum_{k=1}^k W(C_{k})\\right)\\Longrightarrow(1)$\n",
    "\n",
    "$$ \\ \\ \\  where\\ W(C_{k})\\ measures\\ within\\ cluster\\ variations.$$\n",
    "\n",
    "$ b)\\ C_{1}\\ \\cup\\ C_{2}\\ \\cup\\ C_{3}\\ \\cup\\ .......\\cup\\ C_{k}\\ = \\left\\{ 1,2,3,.......,n \\right\\}$\n",
    "\n",
    "$ c)\\ C_{k}\\ \\cap\\ C_{k^\\prime}\\ = \\left\\{ 1,2,3,.......,n \\right\\}$\n",
    "\n",
    "$$ here\\ \\ C_{1},C_{2},C_{3},.......,C_{k}\\ denote\\ sets\\ containing\\ the\\ indices\\ of\\ the\\ observations\\ of\\ each\\ cluster.$$\n",
    "\n",
    "\n",
    "$W(C_{k}) = \\frac{1}{|C_{k}|}\\sum_{i,i^\\prime \\in{C_{k}}}\\sum_{j=1}^d\\left(x_{ij}-x_{i^\\prime j}\\right)^2 \\Longrightarrow(2)$\n",
    "\n",
    "$K-means\\ is\\ based\\ on\\ minimizing\\ the\\ pairwise\\ distance\\ of\\ data\\ points\\ within\\ the\\ same\\ cluster.$\n",
    "\n",
    "$Formally\\ given\\ x_{1},.....,x_{n}\\in R^d,\\ we partition\\ these\\ points\\ into\\ k\\ clusters\\ C_{1},......,C_{k}\\ based\\ on\\ objective.$\n",
    "\n",
    "$Lloyd's\\ algorithm:\\ \\ $\n",
    "\n",
    "$Substituting\\ (2)\\ in\\ (1)\\ :$\n",
    "\n",
    "$$ \\therefore \\ \\ arg\\ min_{C_{1},..,C_{k}} \\left(\\sum_{i,i^\\prime=1}^k\\frac{1}{|C_{k}|}\\sum_{i,j \\in{C_{k}}}\\left|\\left|x_{ij}-x_{i^\\prime j}\\right|\\right|^2\\right)\\Longrightarrow(3)$$\n",
    "\n",
    "$Rewriting\\ in\\ simple\\ terms\\ :$\n",
    "\n",
    "$$ min_{C_{1},..,C_{k}} \\left(\\sum_{l=1}^k\\frac{1}{|C_{l}|}\\sum_{i,j \\in{C_{l}}}\\left|\\left|x_{i}-x_{j}\\right|\\right|^2\\right)\\Longrightarrow(4)$$\n",
    "\n",
    "$Here\\ C_{1},C_{2},C_{3},.......,C_{k}\\ \\Longrightarrow\\ l = 1,2,....,k. Hence\\ written\\ as\\ C_{l}.$\n",
    "\n",
    "$Equation\\ (4)\\ is\\ K-means\\ in\\ a\\ simpler\\ way.$\n",
    "\n",
    "$This\\ cost\\ function\\ is\\ a\\ weighted\\ average\\ of\\ the\\ cluster\\ variances\\ ,with\\ weights\\ proportional\\ $\n",
    "\n",
    "$to\\ the\\ cluster\\ size\\ in\\ terms\\ of\\ number\\ of\\ points\\ |C_{l}| \\approx |C_{k}|$\n",
    "\n",
    "$Let\\ \\  \\mu = \\frac{1}{|C_{l}|} \\sum_{i \\in C_{l}}^n x_{i}\\Longrightarrow(5)$\n",
    "\n",
    "$Splitting\\ (4)\\ and\\ considering\\ the\\ LHS:$\n",
    "\n",
    "$$ \\therefore \\sum_{i,j \\in{C_{l}}}\\left|\\left|x_{i}-x_{j}\\right|\\right|^2\\ = \\sum_{i,j \\in{C_{l}}}\\left(\\left|\\left|x_{i}\\right|\\right|^2 + \\left|\\left|x_{j}\\right|\\right|^2 -2<x_{i},x_{j}>\\right)\\Longrightarrow(6)$$\n",
    "\n",
    "$Applying\\ clusters\\ in\\ (6):$\n",
    "\n",
    "$$ \\sum_{i,j \\in{C_{l}}}\\left|\\left|x_{i}-x_{j}\\right|\\right|^2\\ = \\sum_{i\\in{C_{l}}}\\left(|C_{l}|\\left|\\left|x_{i}\\right|\\right|^2 + \\sum_{j\\in{C_{l}}}\\left|\\left|x_{j}\\right|\\right|^2 -2|C_{l}|<x_{i},\\mu>\\right) $$\n",
    "\n",
    "$$ = 2|C_{l}|\\sum_{i\\in{C_{l}}}\\left|\\left|x_{i}\\right|\\right|^2 - 2|C_{l}|^2\\left|\\left|\\mu\\right|\\right|^2\\Longrightarrow(7)$$\n",
    "\n",
    "$$ \\therefore = 2|C_{l}|\\left[\\sum_{i\\in{C_{l}}}\\left|\\left|x_{i}\\right|\\right|^2 - |C_{l}|\\left|\\left|\\mu\\right|\\right|^2\\right]\\Longrightarrow(8)$$\n",
    "\n",
    "$Considering\\ the\\ RHS:$\n",
    "\n",
    "$$\\sum_{i \\in{C_{l}}}\\left|\\left|x_{i}-\\mu\\right|\\right|^2\\ = \\sum_{i\\in{C_{l}}}\\left(\\left|\\left|x_{i}\\right|\\right|^2 + \\left|\\left|\\mu\\right|\\right|^2 -2<x_{i},\\mu>\\right)$$\n",
    "\n",
    "$$ = \\sum_{i\\in{C_{l}}}\\left(\\left|\\left|x_{i}\\right|\\right|^2 + \\left|\\left|C_{l}\\right|\\right|\\left|\\left|\\mu\\right|\\right|^2 -2\\left|\\left|C_{l}\\right|\\right|\\left|\\left|\\mu\\right|\\right|^2\\right)$$\n",
    "\n",
    "$$ \\therefore \\sum_{i \\in{C_{l}}}\\left|\\left|x_{i}-\\mu\\right|\\right|^2\\ = \\sum_{i\\in{C_{l}}}\\left(\\left|\\left|x_{i}\\right|\\right|^2 -\\left|\\left|C_{l}\\right|\\right|\\left|\\left|\\mu\\right|\\right|^2\\right)\\Longrightarrow(9)$$\n",
    "\n",
    "$Substituting\\ (9)\\ in\\ (8):$\n",
    "\n",
    "$$ \\sum_{i,j \\in{C_{l}}}\\left|\\left|x_{i}-x_{j}\\right|\\right|^2\\ = 2|C_{l}|\\left[\\sum_{i\\in{C_{l}}}\\left(\\left|\\left|x_{i}\\right|\\right|^2 -\\left|\\left|C_{l}\\right|\\right|\\left|\\left|\\mu\\right|\\right|^2\\right)\\right] $$\n",
    "\n",
    "$Rearranging\\ the\\ equation\\ and\\ substituting\\ C_{l} = C_{k} = n,\\ we\\ get:$\n",
    "\n",
    "$$ \\frac{1}{n}\\sum_{i,j \\in{C_{l}}}\\left|\\left|x_{i}-x_{j}\\right|\\right|_{2}^2\\ = 2\\sum_{i\\in{C_{l}}}\\left|\\left|x_{i}-\\mu\\right|\\right|_{2}^2\\ $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
